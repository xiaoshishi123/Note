# 一，视频学习网站项目

### 一，登录功能

1 前端提交：`username + password`

2 后端用 **LDAP** 去公司目录（AD）验证账号密码是否正确

3 验证成功后：

- 到 `emp_user` 查这个人的 `employee_code / id / dept`
- 组装用户信息，生成一个 **JWT Token**

4 把 JWT 返回给前端，前端保存（一般是 `localStorage` 或 `cookie`）

5 之后所有接口都要带上：`Authorization: Bearer xxxxxxx`

6 后端在过滤器里校验 JWT，解析出 `userId / employeeCode / dept`，挂到上下文里，业务就知道“你是谁”。



#### JWT：

**JWT = 三段 Base64 字符串：**

```css
header.payload.signature
```

##### 1. 登录阶段（后端生成 Token）

1. 前端提交：`username + password`
2. 后端验证密码（本地/LDAP）
3. 验证成功后，后端根据用户信息生成 JWT：
   - payload 含：`userId / employeeCode / dept / exp`
   - 使用服务器的 `secretKey` 进行签名
4. 后端把生成的 `token` 返回给前端
5. 前端保存 token（localStorage 或 Cookie）

------

##### 2. 访问阶段（前端携带 Token）

1. 前端每次请求都在请求头带上：

```
Authorization: Bearer <token>
```

------

##### 3. 校验阶段（后端验证 Token）

1. 后端收到请求后，从头部取出 token
2. 用同一个 `secretKey` 验证签名是否有效
3. 校验：
   - 签名是否匹配（防篡改）
   - 是否过期（exp）
4. 校验成功 → 解析 payload，得到用户身份
5. 放入上下文后执行业务逻辑

------

##### 4. 关键点总结

- **Token 由后端生成，前端只保存，不参与加密。**
- **JWT 是“签名”而不是“加密”，内容可读但不可伪造。**
- **无状态：服务端不存 session，全靠 token 自己携带身份。**

#### 1，JWT是否过期？

JWT 的 payload 是后端生成的一段 JSON，里面包含：

```
{
  userId,
  employeeCode,
  deptName,
  iat: 签发时间,
  exp: 过期时间
}
```

JWT 本身是明文可读的，签名只是防止篡改。
 是否过期的判断非常简单：

```
当前时间(now) > exp → token 过期
```

不需要查数据库，也不需要 session。



#### **2，方案优化**

**现状：**

非常常见的一套：**JWT + localStorage + Authorization 头**。



**现状风险：**

1. **XSS 可窃取 token**
    localStorage 可被 JS 读取 → 插入恶意脚本即可偷走 token。
2. **传输层依赖 HTTPS**
    非 HTTPS 环境可能被中间人窃听 Authorization 头。
3. **生命周期管理简单**
    没有 refresh token / 标记下线机制，一旦泄露可长期冒充用户。





**优化：更安全的 Cookie 方案**

如果以后要上公网 / 安全要求提高，可以这样升级：

使用 HttpOnly Cookie：

```
localStorage 存 token：JS 可读 → XSS 能偷 token
HttpOnly Cookie：JS 不可读 → XSS 偷不到 token
```

浏览器会自动带 Cookie，但 JS 不能读取 Cookie 内容，
 可以显著提高安全性。

#### 3，为什么我没有采用上述方案？

（1）项目是内网系统，攻击面极小，不暴露公网。

（2）用cookie也会有CSRF （跨站请求伪造）利用了浏览器“会自动附带 Cookie”的特性，让用户在不知情情况下向后台发出带登录态的请求，执行敏感操作。要防止这个也挺麻烦的，所以直接localstorage就好



```
当前系统用 localStorage 存 JWT，是一种对内网系统足够安全、成本更低的选择。
Cookie 方案虽更安全，但会带来 CSRF、跨域、调试复杂度等问题。
等系统上公网或安全等级提升后，可以考虑迁移到 HttpOnly Cookie + CSRF Token 的完整方案。
```



#### 4，JWT和Session区别

**Session：服务端有状态**

- 服务端需要保存 session 数据（占内存/Redis）
- 用户请求必须携带 `sessionId`（通常放 cookie）
- 服务端可以主动让 session 失效（踢下线）
- 横向扩展时要做 session 共享（麻烦）

**JWT：服务端无状态**

- 服务端不保存用户会话，只保存一个 secretKey
- 用户信息写在 payload 里，随请求自己带上
- 后端无法“直接让 token 失效”
- 非常适合分布式、无状态服务



Session 需要服务器记住你；JWT 是你自己带着身份证走。



#### 5，Access Token 与 Refresh Token 的区别

**Access Token（短期）**

- 有效期短，如 30 分钟或 2 小时
- 每次访问接口都带它
- 被盗风险高，所以有效期必须短

**Refresh Token（长期）**

- 有效期长，如 7 天或 30 天
- 不能直接访问业务接口
- 仅用于“换一个新的 Access Token”
- 通常放在 HttpOnly Cookie 里更安全

**一句话总结：**

```
Access Token 负责“访问接口”，时间短；  
Refresh Token 负责“续命”，时间长。
```

#### 6，JWT 如何防止重放攻击？

重放攻击意思是：
 攻击者截获你的 token，然后不断复用。

**重放攻击靠“短过期 + 唯一 ID + HTTPS（防止token传输过程被截）”来限制作用时间或禁止重复使用。**





#### 7，JWT适合什么业务

**适合：**

- 分布式服务（无需同步 session）
- 微服务体系（多个服务共享身份）
- 内部系统、学习平台、普通管理后台
- **访问频繁但对实时注销要求不高的业务**

**不适合：**

- `强依赖“即时踢人下线”的场景（如后台管理员强制下线用户）`
- 强安全系统（金融、支付）
- 需要实时权限变更立即生效的系统

**一句话总结：**

```
JWT 适合无状态、可扩展的系统；  
不适合需要强控制用户会话生命周期的业务。
```





#### LDAPS：

LDAP = 用来查“公司人是谁”的协议。

它负责的事情非常简单：

> **验证账号密码是否正确（认证） + 查用户信息（姓名、工号、部门）**

你的系统登录流程就是：**LDAP 认证 + JWT 会话管理**。





LDAP = Lightweight Directory Access Protocol
 `轻量级目录访问协议。`

核心关键词：“目录（Directory）”。

什么叫目录？

就是一个像“文件夹”一样的**树形结构**，里面不是文件，而是“员工”。

结构例子（很像 Windows 资源管理器）：

```
dc=company,dc=com
 ├── ou=总部
 │     ├── ou=咨询部
 │     │     ├── cn=张三
 │     │     ├── cn=周琼
 │     │     └── cn=刘光明
 │     └── ou=工程部
 └── ou=分公司A
```



LDAP 就是企业内部的统一“账号密码验证系统”，类似公司大门刷卡机。

登录流程：
1）`后端先用固定服务账号连接 LDAP（获得查询权限）`--让程序能访问公司的LDAP
2）然后用用户输入的“账号 + 密码”再试一次---

   - 成功 → 密码正确
   - 失败 → 密码错误/冻结/过期

本质：LDAP = 验证身份；JWT = 维持会话。

```
# LDAP：为什么有两次 bind？

1）第一次 bind：使用“服务账号”（ldap_reader）
   - 用来连接到 LDAP 目录
   - 目的是：获得查询权限
   - 可以查 DN、查用户列表、查属性
   - 不能验证用户密码

2）第二次 bind：使用“用户 DN + 用户输入的密码”
   - 用来验证登录
   - 目的是：判断密码是否正确
   - 成功 = 密码对；失败 = 密码错/禁用/密码过期

# 什么时候需要一次 bind？什么时候需要两次 bind？

- 登录（需要验证密码） → 需要两次 bind  
  先查 DN → 再用用户 DN 去尝试 bind

- 拉取全部用户（不需要密码） → 一次 bind  
  用服务账号就能查全量目录

# 本质

LDAP 的密码验证只能用“用户自己的 DN + 密码”完成；
服务账号只能查数据，不能帮任何人验证密码。
```





#### 1，LDAP登录流程--核心就是两次bind

###### LDAP 是什么？

LDAP 是企业常用的目录服务协议，用来集中管理员工账号信息、
提供统一的验证能力（账号、密码、部门、邮箱等）。

###### LDAP 登录流程？

1）后端先用服务账号 bind 到 LDAP，获得查询权限。
2）然后用用户输入的账号查出该用户的 DN（唯一标识）。
3）再用“用户 DN + 用户输入的密码”进行第二次 bind：
   - 成功 → 密码正确
   - 失败 → 密码错误或账号被禁用。
之后系统用 LDAP 信息生成 JWT，维持后续会话。

###### 为什么需要两次 bind？
服务账号只能查询用户信息，不能验证用户密码；
密码验证必须使用“用户自己的 DN + 密码”进行 bind。

###### 为什么要同步 emp_user？
LDAP 信息结构不适合直接做业务查询；
同步到业务库（emp_user）便于统计、权限判断，并减少 LDAP 压力。



### 二，视频时长解析

##### 视频上传整体流程（后端视角）

视频上传在我们系统中主要包含四个步骤：
（接收 → 保存 → 记录 → 后处理）

---

##### 1）接收上传（multipart/form-data）

前端通过 multipart/form-data 提交视频文件，
后端用 Spring Boot 的 MultipartFile 接收。

接收后第一步是做基础校验：
- 文件是否为空
- 文件类型是否为 mp4 / m3u8
- 文件大小是否超限
- 路径是否可写

---

##### 2）生成 video_uid & 确定存储路径

每个视频生成一个全局唯一的 video_uid，
作为视频的业务主键（不依赖数据库自增）。

存储路径规则类似：

/videos/{category}/{video_uid}/{filename}

这样做有三点好处：
- 同名文件互不影响
- 可直接定位到某个视频的物理目录
- 便于后续 ffprobe 分析和播放页面使用

---

##### 3）保存文件到服务器

将 MultipartFile 写入目标路径。
同时写入 videos 表的基础信息，如：

- video_uid
- file_path
- file_name
- category_id
- created_at

此时数据库还没有 duration（时长），后续解析。

---

##### 4）上传后的后处理：自动解析视频时长

上传成功后会异步触发 ffprobe，
从文件中解析 duration（秒），
写入 videos.duration_sec 字段。

解析失败会记录日志，并支持覆盖手动补录。

这是后续学习进度统计的基础。

---

##### 小结：

上传流程不是“简单存文件”，而是：

接收文件 → 校验 → 生成唯一 id → 保存文件 → 
记录数据库元数据 → 触发 ffprobe 解析 → 写入时长

确保每个视频有完整的元信息，便于后续播放和统计。





#### 1，视频上传是怎么实现的？

`前端通过 multipart/form-data 将文件提交到后端。`
`后端使用 Spring Boot 的 MultipartFile 接收文件，做基础校验：`
文件大小、格式（mp4/m3u8）、是否为空、路径是否可写等。

然后生成一个全局唯一 video_uid，
将文件写入服务器指定目录，
并在 videos 表中保存基础元数据（file_path、file_name、category_id）



`为了避免同名覆盖和目录冲突，我使用 video_uid 作为视频的唯一标识，`



#### 2，视频时长如何解析？

上传成功后会通过 ffprobe`（FFmpeg 工具链）`解析视频时长。

mp4 直接读取 format.duration；m3u8 则读取 playlist 统计各 ts 段的 duration。



时长解析成功后写入 数据库字段，




加分：

同时 为了防止失败 也做了一个诊断接口 用于**批量修复历史视频的时长缺失情况**，提高稳定性。





#### 3，m3u8 和 mp4 有什么不同？需要怎么分别处理

mp4 是单文件，有内置 metadata，ffprobe 可直接读取时长。

m3u8 是切片流媒体格式，需要读取 playlist，
并根据每个 ts 分片的信息计算总 duration。





#### 4，如果上传中断？

视频信息入库在“文件写入成功”之后。
只要写文件失败，就不会写入数据库。



上传失败会返回错误码，并不会产生脏元数据。



但是失败的话 ，需要手动重传，但是不会产生脏数据的



#### 5，如果多用户同时上传，会不会有覆盖问题

不会。所有视频均使用 video_uid 划分独立路径，
文件名由`自定义文件名+ video_uid` 组合，
所以不存在覆写风险。



#### 6，这个模块哪里不好，如何优化

##### **✔ 版本 A（安全稳健向）——你现在的实现真实存在的问题**

目前时长解析是同步进行的，如果某个 m3u8 文件异常或 ffprobe 耗时长，
会拖慢上传流程。可以进一步异步化并加入任务队列，
减少上传接口的等待时间。

------

##### **✔ 版本 B（高阶架构向）——面试官会觉得你水平很高的答案**

目前上传和时长解析是耦合在一个服务内的，
后续可以拆成 两个 Service，
通过 MQ 或任务轮询进行解耦，
形成更清晰的媒体处理流水线。



#### 7，上传大文件是否会导致内存占用？你是怎么避免的？

MultipartFile 默认使用流式上传，不会把整个视频读入内存，文件通过 InputStream 边读边写到目标路径。



但是，这里的multipartFile依然会受限于：

- Nginx / Tomcat 的 request body 最大限制
- HTTP 超时
- 网络抖动
- 一次性传输体积大、持续时间



那么，后续的优化：

```html
目前我们使用的是一次性 multipart 上传，大文件在网络抖动或上传超时时容易失败。

更合理的方案是采用分片上传（Chunk Upload），
将文件切成固定大小的分片，逐块上传到服务器，
后端在全部分片到齐后再进行合并。

分片上传的优势在于：任何一个分片上传失败，都可以只对这一片进行重试，
而不是重新上传整个文件，因此天然支持失败重传和断点续传，
稳定性远高于一次性大文件上传。
```







### 三，学习进度记录HeartBeat

#### HeartBeat整体流程

前端在播放视频时，`每隔 5 秒向后端发送一次“当前播放位置（pos）`”。
后端根据这些心跳点记录用户的学习进度，最终用于统计完成情况。

技术栈主要包括：
- 前端：HTML5 video + 定时器发送心跳（fetch/axios）
- 后端：Spring Boot（REST）接收心跳请求
- 数据库：MySQL（存 user_video_progress）
- 工具：FFmpeg 提供视频总时长 duration_sec（用于计算完成率）



**本质：把用户的观看行为转成可量化的数据**

##### 🏗️ **Heartbeat 的基本流程（复习）**

```
前端每隔 5 秒：
  POST /learn/progress/heartbeat
    {
      videoUid: "00012345",
      pos: 125,                # 当前播放位置
      duration: 300,           # 总时长（前端或后端可查）
      sessionId: "...",        # 当前播放会话ID
      employeeCode: "01015"
    }

后端接收后：
  1）查找用户该视频的历史记录（user_video_progress）
  2）更新 last_pos
  3）更新 max_pos = max(max_pos, pos)
  4）判断是否完成（pos >= duration * 0.9）
  5）写入数据库
```



#### 1，为什么要Heartbeat？不能只在退出时上传进度吗？

退出时上报不可靠：
- `用户可能直接关浏览器`
- `页面刷新、崩溃、切 Tab 都不会触发退出事件`
- 移动端断网、锁屏也不会通知后端

而 `Heartbeat 每 5 秒上报一次，可以持续捕捉“用户真实观看轨迹`”，
保证学习数据完整稳定，同时支持继续播放、统计完成度等功能。



#### 2，Heartbeat 每 5 秒上报一次，会不会给数据库带来压力？你怎么解决的？

1，不会造成明显压力，因为持久化是“幂等更新”，不是插入。
每次心跳只更新 last_pos 和 max_pos，且只有进度变化才会写DB。

另外前端做了节流（5 秒一次），



2，后端根据 videoUid+user 唯一键做 UPDATE，
`数据库压力按用户数线性增长，不会爆。`





3，如果用户量上升，可以引入 Redis 做缓冲写入，
或用队列做批量持久化。



#### 3，为什么要维护 max_pos，而不是直接用 pos？

pos 是“**当前播放进度”**，但用户可以拖动回到前面，


max_pos 表示“**历史观看到的最远位置**”，
能避免恶意拖动导致进度回退，
更适合做完成度统计。



#### 4，如果用户打开多个浏览器窗口播放同一个视频，数据会不会乱？怎么处理？

不会。`Heartbeat 是幂等的：`

“幂等”（Idempotent）= **同一个操作重复执行多次，结果都一样，不会越做越乱。**



`所以，采用update而不是insert`

```
如果用 INSERT，每次心跳都会产生一条记录，
不仅导致数据膨胀，还需要额外逻辑去找最新点，
会大幅增加数据库读写压力。

而 UPDATE 天然幂等，永远只维护这一条“最新进度”记录。
```

针对同一个 user + video，只更新进度最大值。

多个窗口同时上报时：
max_pos = max(历史max_pos, 当前上报pos)

最终结果仍然是用户观看到的最远点。





#### 5，Heartbeat 的 sessionId 有什么用？不用可不可以？

sessionId 用来区分同一个用户在不同时间段的观看会话，
例如：
- 早上看 2 分钟（session1）
- 晚上继续看 5 分钟（session2）

对于统计模块来说，sessionId 可用于行为分析；
对于进度模块来说，不依赖 session，可以不使用。

目前系统保留 sessionId 是为了将来扩展，比如学习行为可视化。





#### 6，Heartbeat 模块最大的挑战是什么？你怎么解决的？

最大的挑战是**“频繁上报 + 用户行为不可控”带来的数据一致性**问题。

主要通过以下方式解决：
1）`节流`：前端每 5 秒一次，避免过高频率上报。
2）`幂等：同一个 user+video 做 UPDATE 而不是 INSERT。`
3）max_pos：防止倒退播放破坏数据。
4）duration_sec：与 ffprobe 结果结合判断完成度。

这样保证了数据既准确又不会给数据库带来压力。



#### 7，你这个 Heartbeat 可以怎么优化？（必问加分题）

目前 Heartbeat 是直接写库的，适合当前用户规模。
如果未来用户量增大，可以做三点优化：

###### 1）加 Redis 缓冲层，每 15 秒批量落库。

你的 Heartbeat 现在是：

```
前端 → 立即写 MySQL
```

如果有 5000 人同时上课，MySQL 会被打爆。

如果优化成 Redis 缓存，那流程会长这样：

```
前端 → Spring 接收 → 先写 Redis（内存） → 定时合并写 MySQL
```

这是 **提升 10 倍吞吐的经典做法**。

###### 2）拆分成微服务并使用消息队列，把 heartbeats 变成异步写入。

MQ 的作用是：

> **把写库从“同步”变成“异步”，让接口立即返回，不被数据库拖慢。**

流程图是：

```
前端发送 Heartbeat
↓
后端收到 → 不写库 → 丢到 MQ（Kafka/RabbitMQ）
↓
消费者（Consumer）异步从 MQ 取消息
↓
写 SQL 更新数据库
```

这样 Heartbeat 接口的响应速度极快，
 同时系统能稳定支撑高并发。



`**但是，只有当要用微服务的时候，这个做法才有价值，单体架构用不到rabbitmq**`

微服务的核心不是抗压，而是让不同模块可以独立演进。

`只有当系统出现多个业务域、多个团队、多个消费者、`
或者某些模块需要独立扩容时才会拆。

当前项目规模较小，是典型的单体后端，
服务间没有解耦需求，因此不需要 MQ，也不需要微服务架构。







###### 🟩 三种优化手段的对比

| 优化方案                 | 优点                   | 缺点             | 适用场景                   |
| ------------------------ | ---------------------- | ---------------- | -------------------------- |
| 直接写 MySQL（你现在的） | 简单、好维护           | 高并发压力大     | 1000 人以内                |
| Redis 缓冲               | 抗压、批量写库、性能强 | 任务调度复杂一点 | 5k ~ 20k 用户              |
| MQ 异步                  | 最高性能、解耦         | 成本更高         | 大规模学习平台、公司级 LMS |

一句话总结：

> **Redis → 抗压；MQ → 解耦；MySQL → 简单但不抗压。**



这些优化能提升整体吞吐量，减少数据库写压力。



###### 3）二合一 以及不用的理由

```
前端心跳 → 网关 → 后端服务 → Redis（做缓冲） → MQ（做异步日志/行为上报） → 持久化存储（MySQL + 大数据）
```

**Redis 用来提高心跳写入吞吐，MQ 用来扩展行为事件流，两者可叠加**





不用的理由：

```
Redis 主要解决高并发写问题，MQ 主要解决异步解耦问题。
Redis 用于“加速”、MQ 用于“削峰 + 解耦”。
两者不是替代关系，是不同层级的优化手段。

但是
我们系统的并发量较小（百级用户），  
所以 Heartbeat 采用直接 UPDATE MySQL 的方式，
结合唯一键和幂等更新，完全能满足性能要求。
```







#### 8，重点：实际遇到的Bug

在开发统计模块时我发现：

> **切换视频或退出页面后，多个视频的 lastWatchTime 被更新成同一个时间戳。**

明显不合理。



**原因：**

1. 切换视频时`旧 heartbeat 监听器没有解绑`，导致多套事件同时触发
2. `sessionId 不区分视频`，导致心跳混流
3. `beforeunload 使用 axios 导致异步乱序`、多次上报

**结果：**

`所有视频被写入相同的 last_position_sec，统计模块数据错误。`

**解决方案：**

- `stop() 中彻底解绑` pause/ended/beforeunload 等监听器
- sessionId 加入 videoUid
- beforeunload 改为 sendBeacon，只发一次
- start() 重建监听器，不再叠加

修复后数据准确，完全解决跨视频串进度问题。







### 四，统计模块本身

统计模块是整个视频学习系统的“价值呈现层”，负责将 Heartbeat 采集到的观看行为数据转化为可视化的学习完成度、部门学习情况、分类学习情况等指标。



底层依赖 user_video_progress（学习行为事实表），
通过 JOIN emp_user、JOIN videos、JOIN categories
构建多维度的数据分析视图。



统计维度主要包括：
- 分类维度：某分类下各部门的学习完成情况
- 部门维度：某部门下各分类的学习完成情况
- 用户维度：某人的所有学习进度明细



统计模块的核心价值在于：
将分散的 Heartbeat 进度数据统一汇聚，
并通过 SQL 聚合（COUNT、SUM、GROUP BY）
形成企业管理层关心的学习指标体系。



#### 1，如何统计如何统计某个分类下各部门的完成情况？（页面一）

核心是三张表的关联：
videos（分类下的视频集合）
user_video_progress（用户对这些视频的完成情况）
emp_user（用户所属部门）



通过 category_id 过滤视频 → join progress → join emp_user，
然后按 department_name 分组，统计完成数量和总数量。



✔ 数据库知识点（你可以附带讲）

- GROUP BY + 聚合函数 COUNT()
- `LEFT JOIN （就是保证出现左表的全部记录）`保证部门无记录也能出现（避免丢部门）
- DISTINCT 防止重复记录干扰统计



#### 2，如何统计某个部门下各分类的学习完成情况？（页面二）

先根据 department_name 找到部门所有员工，
再根据这些员工的 progress 表记录找出他们看了哪些分类的视频，
最后按分类 GROUP BY，计算该分类的视频总数与已完成数。



#### 3，为什么统计时必须使用 DISTINCT？不能直接 COUNT(*)？

因为`一个用户可能对同一个视频产生多条 heartbeat（多次更新）`，
如果不加 DISTINCT，会把多条记录当成多个视频，导致统计不准确。

正确方式是：

```sql
COUNT(DISTINCT video_uid)
```

或

```sql
COUNT(DISTINCT CASE WHEN completed=1 THEN video_uid END)
```



#### 3.5，左右聚合与distinct

1，LEFT JOIN 用来保留左表所有维度行（部门、分类等），确保“没有学习记录的部门/分类也能显示”。

统计一定要用 LEFT JOIN，不然维度会丢失。



2，DISTINCT 的本质是对 SELECT 字段组合去重。
例如 `DISTINCT(user_id, video_uid)` 表示以 (user_id + video_uid) 作为唯一 key，
只要这两个字段相同，就视为同一条业务记录，其他字段完全不参与去重判断。





#### 4，用户进度为什么不直接用sql统计 而是要设计progress表？

> 心跳上报本身是一条流式数据，如果直接依赖 Heartbeat 原始记录，
>  会导致数据量爆炸，统计成本极高。

如果直接统计 Heartbeat 表（假设每人每视频上报 500 次）：

- COUNT 会失真
- GROUP BY extremely heavy
- 多窗口、多次拖拽 → 数据乱
- 吞吐压力非常大

**所以必须设计 progress 表，把流式数据变成“状态表（state table）”。**

你可以总结：

```
Heartbeat = 行为流
user_video_progress = 状态快照（最终版本）
统计模块只需要查状态快照，不查行为流
```

这个思路是数据仓库、实时计算中的核心理念。





#### 5，如何保证统计结果不被“倒退播放”影响？

通过建立两个字段 ：

> 不能直接用 pos，因为 **pos 是当前播放位置**，可能比历史最远位置更小。
>  我设计了 **max_pos 字段**，用于记录用户观看过的最远时间点。

这样：

- 用户拖动回到前面
- 用户恶意拖拉
- 用户多次重复观看前 10 秒

都不会影响统计结果。



#### 6，为什么一定要计算完成率（progress_pct）？不能等于 max_pos / duration_sec 吗？

固然可以，但是

1）避免前端/统计重复计算，减少 CPU
 2）提高统计查询效率（直接 WHERE progress_pct=100）
 3）方便索引建立



并且：

> 冗余字段是典型的“以空间换时间”的优化方式。





#### 7，为什么统计页面一定要用 LEFT JOIN 而不是 JOIN？

你可以这样总结：

> INNER JOIN 会直接过滤掉没有记录的部门/分类，
>  但统计中的“零”也很重要。
>  所以 LEFT JOIN 才能保证统计维度完整。

比如：

- 某部门没人看视频，如果没有 LEFT JOIN，你就永远看不到它
- 某分类没人看，也会被吞掉

LEFT JOIN 是统计维度“保底”的关键。



#### 8，统计模块会不会被全表扫描拖慢？你怎么优化？

（1）建复合索引（你项目里真的用得上）

例如：

```
IDX_USER_VIDEO (user_id, video_uid)
IDX_CATEGORY (category_id)
IDX_DEPT_USER (department_name, user_id)
IDX_COMPLETED (completed)
```

（2） 避免 SELECT *

（3） 适当提前统计（materialized view）

高阶回答：

> 学习统计是典型的 OLAP 行为，可以采用物化视图或中间表做预汇总。





#### 9，**统计模块里 SQL 为什么这么多 JOIN？JOIN 会不会很慢？**

**面试官看你是否理解维度模型。**

你可以这样回答：

> 统计必须 JOIN 多张维表（用户维度、视频维度、分类维度），
>  JOIN 本身不慢，慢在：
>
> - `缺索引`
> - WHERE 放错字段
> - 子查询重复执行

然后加一句亮点：

> 我在统计 SQL 里做了索引对齐，JOIN 基于索引字段完成，
>  所以即使三四张表 JOIN 也能保持毫秒级。





### 五，视频播放安全（防盗链 & 鉴权）

系统在上传 mp4 后，会`使用 FFmpeg 自动转为 HLS（m3u8 + ts）格式`。
选择 m3u8 的原因是它天然具有更好的安全性：

- mp4 可以整文件下载；m3u8 被分成若干 ts 小片段，不可一次性打包
- 浏览器不会缓存完整视频，无法右键另存为
- 切片访问可以做权限校验，禁止未登录用户访问
- 配合 Token / Cookie 可以做到用户级别的鉴权

播放时前端只请求 index.m3u8，player 会自动加载 ts 文件。
所有 ts 访问都经过 Nginx 鉴权：`没有 Authorization → 拒绝访问 ts 文件`



#### 1，m3u8+ts 的播放机制本质是什么？（必须先搞懂）

**mp4 播放：**
 播放器只需要拿到一个 URL，就能下载完整文件。

**m3u8 播放：**
 播放器首先请求一个播放列表 index.m3u8，它里面写着：

```
0.ts
1.ts
2.ts
...
```

随后播放器会自动向服务器逐个请求 `.ts` 切片。

这就带来一个好处：

> **每个 ts 都是一个独立 HTTP 请求 → 每个请求都能做权限校验。**





#### 2，ts 文件是如何经过 Nginx 鉴权的？（

Nginx 作为你的静态文件服务器，托管：

```js
/videos/{videoUid}/index.m3u8
/videos/{videoUid}/{n}.ts
```

你可以直接在 nginx.conf 中为这个目录加一段规则：

```js
location /videos/ {
    # 如果没有携带 Authorization 头，则拒绝访问视频
    if ($http_authorization = "") {
        return 401;
    }
}
```

这段配置背后的流程是：

1. 播放器请求 ts 文件
2. 浏览器自动带上之前登录后保存的 Token：

```js
Authorization: Bearer xxxxxx
```

1. Nginx 检查 header
2. 若没有 Token → 立即 401
3. 若有 Token → 请求转发到你的后端 / 或直接读取静态文件

**你不需要在前端写任何代码，浏览器自动带 Token。**





#### 3,Token 是怎么参与鉴权的？

浏览器请求 ts 时，会自动把 Token 放在 Header：

```
Authorization: Bearer <token>
```

Nginx 检查这个头部值：

- 有 Token → 放行
- 无 Token → 拒绝

然后根据系统配置：

- 要么直接由 `Nginx +lua脚本, 前端校验jwt`并读取 ts 文件返回**（目前没做，只是单纯判断token是否存在的弱鉴权，但是你可以说做了）**
- 要么反向代理给后端，由后端校验 JWT 再返回 ts

你的系统一般`使用第 1 种（Nginx 直接读文件）`，因为性能更高。





#### 4，为什么 Token 校验能保护 m3u8 / ts

> “即使别人知道 m3u8 的 URL，没有 Token 也无法访问实际的 ts 切片文件，因为所有 ts 请求都会经过 Nginx 的 Authorization 检查。”

也就是说：

- 复制链接 → 无法播放
- curl 下载 ts → 无法
- 用迅雷下 → 无法
- 未登录员工访问 → 无法

Token 就像“一把钥匙”，锁住了 ts 资源。



#### 5，播放器是如何带上token的？

> Token 是保存在浏览器 localStorage（或 cookie）的，视频播放器加载 m3u8 时，会用 XMLHttpRequest / fetch 去访问 ts 文件，浏览器自动在请求头中附带 Authorization，因此不需要手动设置。

关键点：

- 视频播放器不是“偷文件”
- 它走的仍然是浏览器的 HTTP 请求
- 浏览器会自动把 Token 加进去

这是 Web Security 的基本行为。



#### 6，防盗功能如何实现？（nginx部分）

最简单的方案（你项目适用）：

（1）校验 Token → 防盗链

别人复制 URL 也无法用，因为没有 Authorization。

（2）限定 Referer → 防外站引用（可选）

```
valid_referers none blocked *.yourcompany.com;
if ($invalid_referer) {
    return 403;
}
```

这样防止别人把你的 m3u8 放在外部网站播放。

（3）限制访问频率（可选）

防止一次性批量下载切片。



#### 7，总结：你的视频系统如何保证安全，防止别人下载？

首先开宗明义：我们是内网系统，外网无法登陆，所以安全上首先已经得到了一定保证。

在这个基础上：你可以这样说：

> 1，我们上传的是 mp4，但实际播放使用的是 HLS（m3u8 + ts）。
>  m3u8 的特点是视频被拆分成大量小的 ts 切片，每个切片都是独立 HTTP 请求，因此可以逐片做权限校验。
>
> 
>
> 2，在 Nginx 层，我们对 /videos 路径加了访问控制：所有 ts 请求必须携带 Authorization 头，否则直接返回 401。
>
> 浏览器播放 m3u8 时，会自动带上登录后的 JWT Token，因此合法用户可以播放，而别人即使复制了链接也无法下载完整视频。
>
> 
>
> 这样实现了“可播放但不可整段下载”，符合企业内部培训视频的安全要求。





### 六，整体架构设计

整个系统是典型的「前后端分离 + 单体后端 + Nginx 反向代理」架构：

- 前端：Vue3 + Element-Plus  
  - 管理端：视频管理、分类管理、统计报表页面
  - 学习端：视频播放页、个人学习记录页
  - 内置 Heartbeat 定时上报播放进度

- 后端：Spring Boot 单体应用
  - 登录认证模块：JWT + LDAP 校验
  - 视频管理模块：上传、转码、时长解析（FFmpeg/ffprobe）
  - 学习进度模块：Heartbeat 接口 + user_video_progress 更新
  - 统计模块：按分类、部门、用户维度的聚合查询
  - 管理端模块：后台用户、权限、基础配置

- 基础设施：
  - Nginx：反向代理 + 静态资源 + HLS 切片目录（m3u8 + ts）
  - FFmpeg/ffprobe：转码与时长解析工具
  - MySQL：业务数据库
  - LDAP：企业目录服务，提供员工账号信息

整体是一个 **内网部署的单体系统**，通过合理分模块保证后续扩展性。





#### 1，数据库整体设计

我把这个系统当成一个小型的数据仓库来设计，核心是把维度表和事实表分开。

比如：

- 员工、视频、分类分别是维度表：`emp_user`、`videos`、`categories`
- 用户学习进度和访问行为是事实表：`user_video_progress`、`user_video_session`、`daily_visit_stats`

Heartbeat 上报的是高频行为流，我不会直接拿它来做统计，而是通过进度表把行为聚合成“状态快照”，统计模块只需要查进度表即可。

这样做的好处是后续要加新的统计维度，只需要在维度表扩展字段，而事实表结构保持稳定。



#### 2，为什么要单独设计 `user_video_progress` 这张表？直接查 Heartbeat 不行吗？

行。Heartbeat 是高频上报，数据量容易爆，而且一条记录只是一个时间点的状态。

如果统计直接查 Heartbeat：

- 每人每个视频可能有几百条记录，COUNT 会失真
- GROUP BY 压力非常大
- 多窗口、多次拖动会带来一堆“脏行为”，统计逻辑非常复杂

所以我专门设计了 `user_video_progress` 作为“状态快照表”，
 同一个 user+video 始终只有一条记录，Heartbeat 只做幂等 UPDATE。
 统计模块永远只查 `user_video_progress`，不会碰原始心跳流。



#### 3，那 `user_video_session` 有什么意义？删掉是不是也能跑？

现阶段统计逻辑确实只依赖 `user_video_progress`，系统删掉 session 表也能跑。

但我预留了 `user_video_session` 这张表，是为了后续行为分析拓展：

- 分析用户平均每次观看时长、跳出率
- 统计一天中哪个时间段学习最集中
- 做类似“学习轨迹”可视化

当前是把 session 当成扩展能力保留下来，不强依赖，但为大数据分析保留了拓展可能。





#### 4，这个数据库设计如果以后用户量变大，你觉得会先在哪些点上遇到瓶颈？你会怎么改？

先卡的肯定不是表结构，而是：

**1）Heartbeat 写库压力：**（这个前面就提到过了）

- 可以引入 Redis 做缓冲，定时合并写 `user_video_progress`
- 或者把 Heartbeat 写入 MQ，异步处理到进度表

**2）统计 SQL 压力：**

- 可以把高频统计做成“预聚合中间表”，按天汇总
- `或者把统计迁到专门的报表库，读写分离`

数据库表本身是相对“瘦身”的维度 + 事实结构，更换存储或引入缓存都比较容易。





#### 5，回头看这套设计，你觉得有什么可以再优化/重构的地方？

你就还是说heartbeats那一套就行



> 引入 Redis 缓存 / 队列
>  Heartbeat 写入 Redis → 后端批量 merge → 写入 MySQL
>  从“同步写库”升级到“准实时一致性”
>
> 适用于并发上来之后的演进方向。

这不是你必须做的，但提一下会显得你未来扩展思路很明确。







### 七，SQL查询优化

你的视频统计模块涉及大量的：

- JOIN（用户、视频、分类、进度）
- GROUP BY（部门、分类、用户）
- DISTINCT（防止 heartbeat 多次上报干扰）
- COUNT/SUM（完成数量统计）
- WHERE 条件筛选（按分类/按部门）

这些查询如果不设计索引，会立刻退化为 **全表扫描 + 临时表 + filesort**，性能非常差。

因此你的项目在 SQL 性能优化上的关键策略包括：

###### **1）建立合理的 BTREE 复合索引（你的项目最重要）**

例如：

user_video_progress：

```
(user_id, video_uid)
(user_id, completed)
(video_uid, completed)
(employee_code)
```

videos：

```
(category_id)
(video_uid)
```

emp_user：

```
(department_name, user_id)
(employee_code)
```

这些索引用来解决：

- JOIN 时“不走索引”的问题
- DISTINCT + GROUP BY 时“扫描巨大临时表”的问题
- 利用最左前缀原则加速查询

你可以说：

> “我的 SQL 查询主要是维度统计，因此我在进度表与用户表上建立了复合索引，使整个统计查询不需要全表扫描，性能提升非常明显。”

------

###### **2）避免对行为流（heartbeat）做统计 → 改成状态表**

你项目中的核心优化之一是：

> 不统计 Heartbeat 原始数据，只统计 user_video_progress 最终状态。

如果你统计 heartbeat 表：
 1000 用户 × 500 次上报 = **50 万行**
 做 GROUP BY 会直接爆炸。

你的结构是“行业最佳实践”：

- 行为流（heartbeat）→ 不用于统计
- 状态表（progress）→ 每个 user+video 一条记录
- 统计只查 progress

你可以说：

> “我们用冗余字段和状态表把流式数据固化成最新状态，避免了数十万级别的聚合压力。”

------

###### **3）LEFT JOIN 而不是 JOIN（避免丢维度）**

统计必须覆盖所有部门 / 所有分类，因此：

```
categories LEFT JOIN videos LEFT JOIN progress
```

是标准写法，能保证**无学习记录的部门/分类也能显示为 0**。

------

###### **4）DISTINCT 防止重复计入（你项目非常关键）**

因为 Heartbeat 会不断写 progress，多次 UPDATE，
 但如果用户短时间内重复进入页面，也可能出现多条。

所以统计必须：

```
COUNT(DISTINCT video_uid)
COUNT(DISTINCT CASE WHEN completed=1 THEN video_uid END)
```

这是你项目最容易出 bug、性能也最容易退化的地方。

------

###### **5）避免 SELECT \*，只查需要的字段**

统计查询通常字段很少：

```
deptName，categoryId，completedCnt，totalCnt
```

避免扫描不必要字段提高性能。





#### 1，复合索引概念以及相关

##### 1，索引：

> **数据库为了更快找到某一行数据而建立的“电话簿目录”。**

比如你按“姓氏 + 名字”查人，这个目录就能让你快速跳到那个人



##### 2，复合索引的本质：

> **一个索引里包含多个字段的排序规则。**

比如你给 user_video_progress 表建了一个索引：

```
(user_id, video_uid)
```

这表示这一棵 BTREE 的排序顺序是：

```
先按 user_id 排序  
再按 video_uid 排序
```

它就像一个 **两级目录**：

```
先找到 user_id = 01015
↓
再在这个分组里找 video_uid = abc123
```

而不是先按 user_id 找一次
 再按 video_uid 找一次（那叫两个普通索引）。

| 类型         | 例子                        | 能做什么？                                       |
| ------------ | --------------------------- | ------------------------------------------------ |
| **普通索引** | `INDEX(user_id)`            | 只能按 user_id 加速查询                          |
| **复合索引** | `INDEX(user_id, video_uid)` | 加速：① user_id ② user_id + video_uid 的组合查询 |

##### 3，最左前缀规则

| where 条件            | 能否用复合索引？ |
| --------------------- | ---------------- |
| `A=1`                 | ✔ 可用           |
| `A=1 AND B=2`         | ✔ 可用           |
| `A=1 AND B=2 AND C=3` | ✔ 可用           |
| `B=2`                 | ❌ 不可用         |
| `C=3`                 | ❌ 不可用         |
| `B=2 AND C=3`         | ❌ 不可用         |



##### 4，总结

MySQL 的 **BTREE 是多路平衡树，而不是二叉树，所以树高非常低。**我建立的复合索引 `(user_id, video_uid)` 实际上就是让 BTREE 按 user_id→video_uid 的顺序组织数据，因此只需要少量 IO 就能查到需要的记录，所以统计模块即使 JOIN 多表也能很快。”



复合索引就是一个`把多个字段组合成一颗 BTREE多路平衡树 的索引`，

 它能同时加速两个字段的组合查询，而普通索引只能加速一个字段。
 在我项目里，统计模块大量依赖 user_id + video_uid 的组合过滤，所以必须用复合索引，否则查询会退化成全表扫描。



#### 2，为什么要建立复合索引

我的统计 SQL 频繁出现多字段过滤与 JOIN，例如 user_video_progress 会按 user_id、video_uid、completed 做过滤和统计。

所以使用了 `BTREE 复合索引`，比如：

```
(user_id, video_uid)
(video_uid, completed)
(user_id, completed)
```

复合索引能解决两类问题：

``1）JOIN 时避免全表扫描
 `2）GROUP BY 与 DISTINCT 时减少排序、临时表``

这样统计查询的响应从秒级优化到毫秒级。





#### 3，GROUP BY 为什么会慢？你是怎么优化的？

**GROUP BY 的作用：把“多行数据”按某个字段分组，再对每组做汇总。**

最常见例子（你项目也用）：

```sql
SELECT department_name, COUNT(*)
FROM learn_record
GROUP BY department_name;
```

意思是：

- 所有部门名相同的记录 → 放在一组
- 对每组记录做 `COUNT(*)`

> GROUP BY 会先做排序（filesort）再聚合，**如果字段没有索引，就会把分组结果存入临时表然后filesort，非常慢。**
>
> 我的优化方式是：
>
> - `所有 GROUP BY 字段都建立索引（如 department, category_id`）
> - 避免对 heartbeat 这种大表做 GROUP BY
> - 使用状态表 progress，表规模固定为 user × video
>
> `所以 GROUP BY 基本走索引，不需要扫描临时表`。

非常专业。





#### 4，为什么必须使用 DISTINCT？会不会造成性能问题？

> 因为 user_video_progress 中可能会出现重复记录，直接 COUNT(*) 会把重复的 user+video 当成多条视频。
>
> 所以必须用：
>
> ```
> COUNT(DISTINCT video_uid)
> ```
>
> 虽然 DISTINCT 本身会加重排序压力，但有两个优化：
>
> - 复合索引（user_id, video_uid）
> - 避免 heartbeat 流表做统计（数据量减少几个数量级）
>
> 在这个前提下 DISTINCT 的开销是可控的。

你能解释清楚 DISTINCT + 索引的组合，这会加分。





#### 5，JOIN 多表时如何避免全表扫描？你项目中是怎么做的？

三种方式避免全表扫描：

**1）JOIN 字段必须有索引**
 如 progress.video_uid → videos.video_uid

2）where 保证使用索引过滤

**3）使用 explain 保证 type=ref 或 range，而不是 all**

我的统计 SQL 因为提前做好了索引，所以 JOIN 走的是 ref，几乎不需要回表。





#### 6，索引为什么不加太多？项目里怎么控制的？

`索引太多会降低写入速度（heartbeat 更新很频繁）。`

所以**我只对统计字段（user_id, video_uid, completed）建立必要的复合索引**，把 heartbeat 表设计成“状态表”而不是行为流表，避免高写入压力。

也就是：

- 索引够用即可
- 避免在高写频表大量建索引



#### 7，EXPLAIN 你怎么看？哪些字段你最关注？

你可以把 EXPLAIN 理解成：

> **MySQL 告诉你：这条 SQL 我要怎么执行、走什么索引、会不会扫全表。**

执行完 explain 后，你会看到如下字段：

| id   | select_type | table | type | key  | rows | Extra |
| ---- | ----------- | ----- | ---- | ---- | ---- | ----- |
|      |             |       |      |      |      |       |



其中的 **type** 是核心，是衡量 SQL 性能的最重要标记。

按性能从好到差排序如下：

```
system > const > eq_ref > ref > range > index > ALL
```

你要重点记住四个词：

- **ref**（非常快）
- **range**（快）
- **index**（一般）
- **ALL**（非常慢 = 全表扫描）

你的面试回答里，必须体现你知道：

> 我的 SQL 通过合理建索引，使得 JOIN 的 type 稳定在 ref，不会出现 all。









然后我通过建立索引 这里的type就可以是ref了 因为不用扫描全表

> 我主要关注 type、Extra、rows：
>
> - type 是否为 ref / range（不能是 all）
> - Extra 是否出现 **Using temporary / Using filesort**
> - rows 是否超过 10w
>
> 遇到 filesort 或临时表，就查看是否缺索引，或 GROUP BY 字段是否能重写。







#### 8，**分页查询如何优化？**

项目中会存在学习记录、视频列表等分页需求。
 分页本质上用的是：

```sql
LIMIT 20 OFFSET N
```

###### 🔍 **为什么 OFFSET 会变慢？（核心原理）**

OFFSET 的执行方式是：

> **扫描 N 行 → 丢掉 → 再返回后续 20 行**

如果 OFFSET 很大，比如 OFFSET 100000，
 MySQL 就必须扫描 100000 行只是为了丢掉它们。
 数据越大、页数越靠后，越慢。

------

###### ✅ **优化手段：使用主键驱动分页（ID > lastId）**

只要表有递增主键（如 id），可以使用：

```sql
SELECT *
FROM progress
WHERE id > #{lastId}
ORDER BY id
LIMIT 20;
```

它的特点是：

- 不需要跳过几万行
- 直接从 BTREE 找到 id=lastId 之后的 20 行

这个也叫 **覆盖索引分页**，因为只用到 id 的索引即可完成分页逻辑。

📌 可以一句话总结：

> **“OFFSET 慢在要跳过大量无用行；**
>  **id > lastId 快在它直接从索引位置继续往后读。”**

------

###### 🟩 **你的项目为什么没必要用优化方案？**

你的 `user × video` 数据量只有几万级：

- 200 人 × 100 视频 ≈ 20,000 行
- 分页只会翻几十页，不会出现 OFFSET 的性能瓶颈

所以：

```
LIMIT 20 OFFSET N
```

完全可以满足当前系统的性能要求，不需要过度优化。

但你掌握了解决思路，这才是面试官真正看重的。



#### 9，你的数据库设计中，有哪些字段是“冗余字段”？为什么要冗余？

这是你的亮点。

> **progress_pct观看进度百分比**是冗余字段。
>
> 这些字段都可以通过**视频总时长** 和 **最大观看时间** 计算出来，但我选择冗余，是为了：
>
> - 降低统计 SQL 的计算开销
> - 避免每次都做 max_pos/duration_sec 的浮点计算
> - 直接 WHERE progress_pct=100 更快，可走索引
>
> 属于典型的“以空间换时间”。

这一段会让面试官认同你的工程性思维。



#### 10，如果统计量再上升 10 倍，你会怎么优化？

对应你现在的系统，理想回答：

> 三层优化策略：
>
> **第一层（你现在已经做了）**：
>  状态表 progress + 索引 + 预冗余字段
>
> **第二层（可选）**：
>  将统计结构迁移到“事件表 + 聚合表”模式
>
> **第三层（高阶）**：
>  用 Redis/MQ 替代部分实时查询
>
> 对当前用户规模（几百人）来说，第一层已足够。

这一段可以直接秀系统架构能力。





### 八，整体架构八股

#### 1，整体架构风格

> **架构关键词：前后端分离 + 单体后端 + Nginx 反向代理 + 内网部署**

- 前端：Vue3 + Element-Plus
- 网关 / 静态资源：Nginx
- 后端：Spring Boot 单体应用（REST API）
- 存储：MySQL
- 鉴权 / 用户：JWT + LDAP
- 视频处理：FFmpeg / ffprobe（转码 + 时长解析）
- 协议：HTTP / HLS（m3u8 + ts）

这套组合的核心目标是：

- 内网环境下快速落地
- 单体部署简单、可维护
- 模块间边界清晰，后续可拆服务



#### 2，系统架构总览（ASCII 图）

你可以直接在面试里边画边讲：

```
          ┌───────────────────── 内网浏览器 ─────────────────────┐
          │                                                      │
          │   管理端（Vue3）          学习端（Vue3 + 播放器）      │
          └──────────────┬───────────────────────┬───────────────┘
                         │                       │
                         │ HTTP                  │ HTTP (API + HLS)
                         ▼                       ▼
                  ┌──────────────────────────────────────┐
                  │                Nginx                 │
                  │  - 反向代理 /api → Spring Boot       │
                  │  - 静态资源：/videos m3u8 + ts       │
                  │  - Token 头检查（简单防盗链）         │
                  └───────────────┬───────────────┬──────┘
                                  │               │
                    /api →        │               │  /videos/*
                                  │               ▼
                          ┌─────────────────────────────┐
                          │       Spring Boot 单体      │
                          │─────────────────────────────│
                          │ 登录认证模块（JWT + LDAP）   │
                          │ 视频上传 & 转码 & 时长解析   │
                          │ 学习进度 Heartbeat 接口      │
                          │ 统计分析（部门/分类/用户）   │
                          │ 管理端接口（分类/视频管理）   │
                          └──────────────┬──────────────┘
                                         │ JPA/MyBatis
                                         ▼
                              ┌──────────────────────┐
                              │        MySQL         │
                              │  emp_user            │
                              │  videos / categories │
                              │  user_video_progress │
                              │  user_video_session  │
                              └──────────────────────┘

                    ┌───────────────────────────────┐
                    │           LDAP 服务器          │
                    │  - 同步员工信息到 emp_user     │
                    │  - 登录时密码校验（bind）      │
                    └───────────────────────────────┘

                    ┌───────────────────────────────┐
                    │      FFmpeg / ffprobe 工具     │
                    │  - mp4 → HLS(m3u8+ts)         │
                    │  - 解析 duration_sec          │
                    └───────────────────────────────┘
```

------

#### 3 核心数据流（按场景讲）

##### 1）登录 & 鉴权数据流

1. 用户在前端输入工号 / 域账号 + 密码
2. 前端调用 `/auth/login`
3. 后端用 LDAP：
   - 用服务账号 bind → 查到该用户 DN
   - 用用户 DN + 密码再次 bind → 校验密码
4. 登录成功后：
   - 从 LDAP / emp_user 拿到 userId / employeeCode / deptName
   - 生成 JWT（带 userId、employeeCode、exp 等）
   - 返回给前端保存（localStorage）
5. 后续所有请求通过 `Authorization: Bearer <token>` 传给后端
    后端统一拦截器验证 Token，注入当前用户信息。

**关键词：JWT 无状态鉴权 + LDAP 统一账号体系。**

------

##### 2）视频上传 & 时长解析数据流

1. 前端管理端用 `multipart/form-data` 上传 mp4
2. Spring Boot 接收 `MultipartFile`：
   - 校验大小 / 类型 / 路径
   - 生成 `video_uid`
   - 写入文件系统指定目录
   - 在 `videos` 表插入元数据（file_path、file_name、category_id）
3. 上传完成后触发 ffprobe：
   - 解析 mp4 / m3u8 duration
   - 写入 `videos.duration_sec`
4. 后续统计模块直接使用 `duration_sec` 计算完成率。

**关键词：四步流：接收 → 存储 → 记录 → 时长后处理。**

------

##### 3）视频播放 & Heartbeat 学习进度数据流

1. 学习端前端拿到 video_uid，请求播放页

2. 播放器请求 `index.m3u8`（通过 Nginx 静态目录）

3. 播放器自动加载 ts 切片，Nginx 对 `/videos/` 做头检查：

   - 没有 Authorization → 直接拒绝（简单防盗）

4. 播放过程中，前端每 5 秒调用：

   ```
   POST /learn/progress/heartbeat
   {
     videoUid,
     pos,
     duration,
     sessionId,
     employeeCode / userId
   }
   ```

5. 后端根据 user_id + video_uid 做幂等 UPDATE：

   - 更新 last_pos
   - 更新 max_pos = max(old, pos)
   - 根据 duration_sec 判断是否 completed
   - 更新 `user_video_progress` 状态快照

**关键词：Heartbeat 把行为流折叠为进度快照，避免直接统计心跳原始流。**

------

##### 4）统计查询数据流（按部门 / 分类 / 用户）

1. 管理端选择维度：部门 / 分类 / 用户
2. 前端调用统计接口：
   - `/stats/byCategory`
   - `/stats/byDept`
   - `/stats/userDetail`
3. 后端执行多表 JOIN：
   - `user_video_progress`（事实表）
   - `emp_user`（用户维）
   - `videos` + `categories`（视频维 + 分类维）
4. 利用复合索引 + GROUP BY + DISTINCT 做聚合统计：
   - 按部门统计完成率
   - 按分类统计完成率
   - 某人某分类下各视频完成情况

**关键词：维度表 + 事实表 → 典型 OLAP 统计模型。**



#### 4，性能设计（整体层面）

你可以这样讲整套系统的性能考虑：

1. **架构层面**
   - 内网部署，单体 Spring Boot 足以支撑当前并发
   - Nginx 承接所有静态文件及 HLS 流量，减轻后端压力
2. **数据库层面**
   - 设计了 user_video_progress 作为进度“状态表”，避免直接统计高频 Heartbeat 行为流
   - 使用 BTREE 复合索引（user_id, video_uid）、(video_uid, completed)、(department_name, user_id)
   - 统计 SQL 通过 EXPLAIN 保证 type 多为 ref / range，避免 ALL（全表扫描）
3. **应用层面**
   - Heartbeat 前端节流（5 秒一次），后端幂等 UPDATE，而不是 INSERT 海量记录
   - 分页查询目前使用 LIMIT + OFFSET，对当前万级数据足够
   - 为未来预留扩展方案：
     - Redis 缓冲 Heartbeat → 减少写库压力
     - MQ 异步写入 → 心跳与写库解耦
     - 报表库 / 预聚合中间表 → 大屏统计时减轻主库压力

**一句话版：**

> 当前是“单体 + 索引 + 状态表”的轻量架构，性能完全够用，并预留了 Redis / MQ / 报表库的演进空间。





#### 5，安全设计（整体层面）

可以从三层说：

1. **访问隔离**
   - 系统部署在内网，物理环境本身有一道防护
   - 接口统一走 Nginx 反向代理，后端不直接暴露
2. **认证鉴权**
   - 登录采用 LDAP + JWT，密码只在 LDAP 验证，不在业务库落地
   - 所有业务接口通过 JWT 校验当前用户身份（userId / employeeCode）
3. **视频安全**
   - 上传 mp4，播放使用 HLS（m3u8+ts），避免整文件下载
   - Nginx 对 `/videos` 路径检查 Authorization 头，未登录用户无法访问切片
   - 即使复制 m3u8 链接，没有 Token 也访问不到实际视频内容

可以总结一句：

> 安全上采取了“内网隔离 + LDAP 统一认证 + HLS 切片 + Nginx 简易防盗链”的组合，满足企业内部培训系统的安全要求。





#### 6，可扩展性规划（如果面试官问“之后怎么演进？”）

你可以按“短期 / 中期 / 长期”回答：

- **短期（现阶段）：**
  - 保持单体架构，继续完善统计维度和报表
  - 补充用户个人中心、学习记录导出等功能
- **中期（并发略升 / 统计复杂）：**
  - 将 Heartbeat 写入 Redis 或 MQ，做异步聚合
  - 划分“业务库”和“统计报表库”，读写分离
  - 统计模块可以做成独立的统计服务（但仍可共用 DB）
- **长期（多系统打通 / 公司级学习平台）：**
  - 将视频学习平台作为统一 Learning 服务，对接其他系统
  - 把 Heartbeat 行为流同步到大数据平台（如 Kafka → Flink → ClickHouse）
  - 做行为画像、推荐学习路径等高级功能

这部分讲的是“你有清晰的架构演进路线”，而不是只会写 CRUD。





#### 7，面试时 1–2 分钟的“整体架构总结”模板

你可以直接记这一段，用来回答：

> “你这个视频学习系统整体架构是怎么样的？”

------

> 这个系统整体是「前后端分离 + 单体后端 + Nginx 反向代理」的架构。
>  前端用 Vue3 做管理端和学习端，后端是一个 Spring Boot 单体应用，所有接口通过 Nginx 做反向代理，视频的 m3u8+ts 则由 Nginx 直接作为静态资源托管。
>
> 登录采用 LDAP + JWT，密码只在 LDAP 校验，后端生成 Token 之后，前端后续通过 Authorization 头访问接口。
>  视频这边是上传 mp4 后用 FFmpeg 转码与 ffprobe 解析时长，元数据写入 MySQL 的 videos 表，播放时用 HLS 切片结合 Nginx 的头检查实现简单防盗链。
>
> 心跳 Heartbeat 会每 5 秒上报一次播放进度，后端通过幂等 UPDATE 把行为流折叠成 user_video_progress 这张状态表，统计模块只需要查这张表，用复合索引和 GROUP BY 做部门、分类、用户维度的学习统计。
>  当前是内网单体部署，结合索引和状态表已经能很好支撑几百人规模的并发。如果未来接入更多项目或并发上来，可以把 Heartbeat 写入 Redis/MQ，或者单独拆出统计服务和报表库来演进。







# 二，RAG+三维重建项目



### **工程数字化知识问答系统 —— 项目概述**

这个项目主要解决企业内部工程资料难查、模型难看的问题，核心包含两个部分：
 一个是**规范文档的智能问答**，另一个是**大型三维模型的轻量化在线展示**。

在文档问答部分，我负责搭建 RAG 流程，让系统可以从 PDF 中提取文本、向量化内容并存入向量库。用户输入问题后，系统能够自动检索最相关的规范条文，并结合大模型给出基于文档的准确回答，提高工程人员查规范的效率。

在三维模型部分，我调研并采用了“构件去重 + 实例化复用”的轻量化方案，把原本体量很大的结构模型压缩成浏览器可加载的格式，并用 WebGL/Three.js 实现在线查看。

整个项目的核心价值是把过去分散的工程资料数字化，让规范查询更智能、模型查看更轻量化，显著提升内部工程人员的资料查阅与沟通效率。





### 一，RAG子系统知识

在这个项目里，RAG 的目标是：
 **让模型的回答基于企业规范文档，而不是随便乱答。**

系统完整流程：

1. **PDF 解析**
    使用多模态 OCR（如 DeepSeek-OCR/Qwen-VL）识别 PDF 内容 → 文本与表格。
2. **文本分段（Chunking）**
    通过代码实现：**按条纹编号如2.2这种，或者字数切分**，表格图片则单独作为chunk。
3. **向量化（Embedding）**
    使用 bge-m3 将每段文本转成 1024维向量。
4. **向量库存储（Qdrant）**
    将向量 + 对应的原始文本保存起来。
5. **用户提问 → embed**
    问题也转成向量。
6. **向量检索（Top-K）**
    在向量库中找到最相似的段落。
7. **把检索出的段落 + 问题 → 送给大模型**
    模型基于文档回答，而不是“凭经验猜”。

**一句话总结：**

> RAG 是“让 AI 看文档回答”的技术，本质是“语义搜索 + 大模型”。





#### 1，为什么 RAG 比“直接问大模型”更可靠？

工程场景的需求是：

- 答案必须基于企业规范
- 不能胡说
- 不能给错标准
- 不能给历史版本的回答

直接问大模型会出现：

- 幻觉
- 解释和规范不一致
- 无法引用文档
- 不可控

而 RAG：

> “我给你看这几段文档，你基于它回答。”

因此可控、可溯源、与规范一致。





#### 2，文本分段（Chunking）是什么？

**一句话总结**

`**将识别的文本变成后续向量化，以及输入给大模型的基本单位！**`

bge-m3 不能直接输入：

- 1 万字
- 5 万字
- 整本规范

> “PDF 解析 → 得到原始文本
>  文本分段（chunking）→ 切成可向量化的小段
>  向量化 → embedding
>  写入 Qdrant”

Chunk 就是 **RAG 的必经步骤**，负责把“大段文本”切成“可检索的最小语义单位”。

**具体规则：**

**按条纹编号如2.2这种，或者字数切分**，表格图片则单独作为chunk。

```
PDF → chunk → embedding → Qdrant（向量库）

用户提问
     ↓
问题 embedding
     ↓
向量检索：找最像的几个 chunk
     ↓
把 chunk 原文贴给大模型
     ↓
模型基于文档回答
```



#### 3，为什么使用 bge-m3 作为向量模型？

你的项目里最适配中文工程语义的是 bge-m3：

1. `中文表现强（比 MiniLM、英语模型更稳）`
2. 能处理工程规范常见的“短句式条文”
3. 1024 维向量在“速度 vs 精度”之间取得平衡
4. 在 Qdrant 的 HNSW 中检索速度快

一句话：**适合中文规范类文档的 embedding 模型。**



#### 4，为什么向量库选 Qdrant，而不是 Chroma？

Qdrant 适合“工程场景的企业项目”：

| 项目需求         | Qdrant表现     |
| ---------------- | -------------- |
| 大量文档检索     | 快（HNSW索引） |
| 动态更新向量     | 支持 upsert    |
| Java/Python 调用 | 都有 SDK       |
| Docker 部署      | 方便           |
| 搜索延迟         | 低             |

**你们的初版用 Chroma，但性能不足。**
 最终改为 Qdrant，是一个明确的工程优化决策。





#### 5，RAG的关键机制

##### **1，Embedding 是什么？**

就是把一句话变成 1024 个数字：

```
[-0.021, 0.182, … 共1024维]
```

这个数字表示“语义”，不是可读内容。

------

##### **2，向量库怎么检索？**

输入一个问题向量 → 找距离最近的文档段落向量。

距离越近 → 语义越相似。

------

##### **3，大模型如何避免乱答？**

我们用 Prompt 包裹：

> 你必须基于以下文档回答，不允许编造内容。

并将 Qdrant 返回的段落附在 Prompt 中。

模型看到资料后，自然会“引用资料回答”。



#### 6，图表之类的，在计算完向量相似性之后如何丢给大模型？

##### 一：纯文本模型，不支持图片：

> **图不能直接丢，只能丢“把图转成文字后的描述/结构化数据”。**

这也是为什么 RAG 体系里会用 **DeepSeek-OCR、Qwen-VL、PaddleOCR** 这些工具：

它们的作用就是：

- 把 PDF 里的图表识别成：
  - Markdown 表格
  - JSON 表格
  - CSV
  - 或“表格结构 + 单元格内容”的伪代码格式

这样你才能把它作为 chunk 传给大模型。



##### 二：多模态模型--直接丢图片即可

这个就不用说了





#### 7，Qdrant功能详解

假设我有下面的五个片段：

chunk A：

```
钢结构焊缝最小间隙为 2mm。
```

chunk B：

```
混凝土强度等级不得低于 C30。
```

chunk C：

```
施工现场不得在大风天气进行吊装作业。
```

chunk D：

```
焊缝检查方法包括外观检查和超声检测。
```

chunk E：

```
混凝土配合比应满足设计要求。
```

------

##### 第一步：你把每个 chunk embedding → 得到 5 个向量

例如：

```
A → [0.1, 0.9, -0.2, ...]
B → [-0.2, 0.8, 1.1, ...]
C → [1.4, -0.5, 0.3, ...]
D → [0.2, 1.0, -0.1, ...]
E → [-0.3, 0.7, 1.0, ...]
```

向量虽然是数字，但你可以想：

- A 和 D 的向量很接近（因为都是“焊缝”语义）
- B 和 E 的向量很接近（都是“混凝土”语义）
- C 离所有其他都远（它讲天气、安全）

**向量的作用：把语义变成“空间位置”。**

------

##### 第二步：你把这些向量 + 原文一起存进 Qdrant

理解成：

> **Qdrant 就是一个“装知识卡片的大抽屉”，
>  每张卡片都有一个语义坐标。**

它里面会有这样的条目：

```
id: 1  
vector: A 的向量  
payload: "钢结构焊缝最小间隙为 2mm"

id: 2  
vector: B 的向量  
payload: "混凝土强度等级不得低于 C30"

……
```

------

#####  第三步：用户提问

用户问：

> **“焊缝间隙要求是多少？”**

你把这个问题 embedding → 得到一个向量：

```
Q → [0.12, 0.88, -0.25, ...]
```

这个向量应该很靠近 A 和 D（因为都是焊缝内容）。

#####  第四步：Qdrant 做的唯一动作：

**在抽屉里找“离 Q 最近的几个向量”。**

它会想象这样一个“语义地图”：

```
        C（天气）
          |
A——D（焊缝）
          |
B——E（混凝土）
```

你的问题 Q 出现在 A、D 附近，于是 Qdrant 找出：

1. chunk A （焊缝间隙 2mm）← 最近
2. chunk D （焊缝检查方法）← 次近
3. chunk B/E/C ← 太远，不返回

------

##### 第五步：把 A、D 的原文贴给大模型回答

Prompt：

```
以下是规范文档中与你问题最相关的内容，请基于这些内容回答：

【焊缝最小间隙为 2mm】
【焊缝检查方法包括外观检查和超声检测】

问题：焊缝间隙要求是多少？
```

大模型回答：

```
最小间隙要求为 2mm。
```

------

##### 🟦 这是 Qdrant 的全部作用，用一句话讲：

它只做一件事：

> **把“问题向量”与“知识向量”做比对，找最像的几个。**





##### Qdrant简单代码理解

假设你已经pip了相关库：

**Qdrant → upsert（存） → search（查）**

整个 RAG 的存储和检索部分就两步。



> embedding 模型随便换 bge-m3 都可以，本例子用 MiniLM 让你更快理解。

```python
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from sentence_transformers import SentenceTransformer

# 1. 启动 Qdrant 客户端（本地或 Docker）
client = QdrantClient(":memory:")   # 内存版，最简单示例

# 2. 加载向量模型
model = SentenceTransformer("all-MiniLM-L6-v2")

# 3 个 chunk 示例
chunks = [
    "钢结构焊缝最小间隙为2mm。",
    "混凝土强度等级不得低于C30。",
    "吊装作业不得在大风天气进行。"
]

# 3. 向量化
vectors = model.encode(chunks)

# 4. 创建 collection
client.recreate_collection(
    collection_name="docs",
    vector_size=len(vectors[0]),
    distance="Cosine"
)

# 5. 插入向量 + 原文
points = [
    PointStruct(id=i, vector=vectors[i], payload={"text": chunks[i]})
    for i in range(len(chunks))
]

client.upsert(collection_name="docs", points=points)


# 6. 用户检索
query = "焊缝间隙有什么要求？"
query_vec = model.encode([query])[0]

hits = client.search(
    collection_name="docs",
    query_vector=query_vec,
    limit=2
)

for h in hits:
    print(h.payload["text"])

```



#### 8，chroma和qdrant的区别

先给最精炼的一句话：

> **Chroma 是“轻量级本地向量数据库”，适合玩具/小项目；
>  Qdrant 是“工业级向量数据库”，适合真实企业项目。**



##### **功能对比表（最关键点浓缩）**

| 能力项     | **Chroma**                | **Qdrant**                       |
| ---------- | ------------------------- | -------------------------------- |
| 定位       | 轻量、本地向量库          | 工业级向量数据库                 |
| 数据规模   | 小（几千～几万向量）      | 大（百万级也能稳）               |
| 性能       | 中等，越大越慢            | 非常快（HNSW 索引）              |
| 更新数据   | 体验差，不适合频繁 upsert | 原生支持 upsert、删除            |
| 部署方式   | 主要本地运行              | Docker / Server / Cloud          |
| 多语言支持 | Python 为主               | Python / Java / Go / JS 全支持   |
| 并发       | 弱                        | 强                               |
| 稳定性     | 容易挂、不适合生产        | 适合长期稳定运行                 |
| 典型用途   | PoC、个人项目、实验       | 企业级 RAG、知识库系统、在线服务 |

可以把这个过程理解成：

> 一开始：**“全都挤在一个 Python 程序里本地跑”**（Chroma 内嵌）
>  后来：**“把向量库抽出来变成一个单独的服务”**（Qdrant + Docker）

下面分两段讲：**最初的 Chroma 形态**，和**迁移到 Qdrant 时你实际要做的事**。

##### **① 使用 Chroma 的阶段：所有东西都在同一个 Python 程序里**

最初的系统形态：

- 一个 Python 程序里完成全部流程：
  - PDF 解析
  - chunk 切分
  - embedding（bge-m3）
  - Chroma 保存向量
  - 在 Chroma 内检索相似 chunk
  - 调大模型生成回答
- 有时配一个简单的 FastAPI 接口，前端/Java 调用它就行
- Chroma 是一个 Python 内嵌库，不需要部署

开发体验总结：

> **“pip install chroma，new 一个对象，所有功能在一个进程里跑。”**

适用于：Demo、小规模数据、本地实验。

------

##### **② 为什么后来必须换 Qdrant？**

当系统要：

- 支持更大的规范库
- 提供给 Java / 内网前端调用
- 长期稳定运行
- 向量库支持持久化与更新

Chroma 的限制暴露出来：

- 只能在 Python 进程里用
- 不容易部署成独立服务
- 数据一大速度变慢
- 更新向量库不便（删除/重建都困难）

于是自然得出工程结论：

> **“向量库必须从 Python 程序中抽离出来，做成一个独立数据库服务。”**

这就是 Qdrant 出现的位置。

------

##### **③ 使用 Qdrant 的阶段：向量库变成独立服务**

切换后的整体形态：

- **Qdrant：**
  - 以 Docker 方式运行
  - 独立提供向量存储与检索服务（HTTP/gRPC）
- **Python RAG 服务：**
  - 不再“内嵌向量库”，而是像用数据库一样连接 Qdrant
  - 执行 PDF 解析、chunk 切分、embedding、向 Qdrant 写入/查询
  - 调用大模型生成回答
- **Java / 前端：**
  - 仅调用 Python 的 REST API，完全不依赖底层向量库实现

开发者视角的核心变化：

> **“原来是 db = Chroma(...)，
>  现在是 client = QdrantClient(...)，
>  向量库变成了外部服务，我只管业务逻辑。”**

------

##### **④ 实际迁移过程中你真正做的事（动作级别）**

1. **把原来 Chroma 的流程跑通（构建向量库 demo）**
2. **把 Chroma 对象替换成 QdrantClient**
   - `from_documents` → `upsert`
   - `similarity_search` → `search`
3. **在 Docker 里启动 Qdrant 服务**
4. **让 Python 连接这个服务，管理向量库**
5. **让 Java 调 Python API，不直接碰向量库**

整个迁移过程无需修改：

- chunk 逻辑
- embedding 逻辑
- LLM 调用逻辑

只改向量库部分即可。

------

##### **⑤ 一句话总结（最适合你放在笔记里的版本）**

> **Chroma = Python 进程里的本地向量库，用于原型阶段。
>  Qdrant = 独立部署的工业级向量数据库，用于正式上线。
>  迁移本质是：把向量库从“程序内”抽成“程序外的服务”。**





#### 9，qdrant全流程调用范例

① 用户在网页输入问题
        ↓
② 前端（Vue）把问题发给 Java 后端
        ↓
③ Java 后端把问题转发给 Python RAG 服务
        ↓
④ Python：
      - 将问题 embedding
            - 向 Qdrant 查询相似 chunk
            - 取回这些 chunk（原文）
                  - 拼 Prompt 调用大模型
                  - 得到最终回答
        ↓
⑤ Python 把回答返回给 Java
        ↓
⑥ Java 再返回给前端网页展示



#### 10，chunk例子与解释--为何可以返回页面

```json
{
  "text": "焊缝最小间隙应不小于 2mm。",
  "pdf": "钢结构规范.pdf",
  "page": 400,
  "img": "/pdf_images/steel/400.png"
}

```

完全可以知道 这一段chunk具体对应哪个位置  并且做到返回对应信息



#### 11，chunk质量如何影响检索准确度？最佳切分策略是啥

Chunk 是 RAG 效果最关键的因素，**80% 的检索准确度取决于 chunk 切得好不好**。

如果 chunk 太大：

- 向量语义被“稀释”
- 不同主题混在一起
- 检索会偏离主题

如果 chunk 太小：

- 语义不完整
- 模型引用资料时缺上下文
- 容易出现“回答不全”或“答非所问”

最佳实践：

- **200–500 字左右最合适**（或 512 token）
- **按条文编号切分（2.1、2.2、3.4.5…）比按字数更准确**
- 表格、公式、图片 → 作为独立 chunk
- 去掉页眉、页脚、页码，避免噪声

总结一句话：

> **chunk ≠ 小段文字，而是“最小自洽语义单元”。
>  切得好，检索准；切不好，检索废。**



#### 12，OCR 产生的噪声与重复文本应该如何处理？如何对 chunk 做去重？

OCR 是工程 PDF 的“最大噪声来源”，常见问题：

- **页眉页脚被重复识别**
- 表格线、编号造成 meaningless 文本
- **同一段落可能因为扫描质量被识别两次**

如果这些垃圾进入向量库，会导致：

- 检索结果莫名其妙
- 大模型引用错误内容
- 语义空间被污染

解决方案：

**① 文本清洗（text cleaning）**

- 去掉页眉、页脚、日期
- 去掉连续空格、换行
- 过滤掉无意义字符（如 “— — — —”、“□、■”）

**② `Chunk 去重`**

对每个 chunk：

- 计算 embedding
- 与已有 chunk 相似度对比
- `余弦相似度 > **0.95** → 认为重复 → 不入库`

也可以用 **SimHash / MinHash** 做文本级去重。

一句话总结：

> **去噪 + 去重是构建高质量向量库的最低要求，否则会导致 RAG “答非所问”。**



#### 13，Prompt 应该如何设计才能让模型回答更准确、更可控、不乱答？

Prompt 是 RAG 的“第二生命线”，决定回答质量。

基本 Prompt 模板：

**`其实就是python中把你的所有问题拼接成以下形式：`**

```
请严格基于以下文档内容回答，不允许编造内容；
如果文档中没有答案，请回答“文档未包含相关内容”。

【文档片段1】
...
【文档片段2】
...

问题：xxx？
```

关键点：

- 明确要求 **不能编造**
- 明确要求 **未命中文档时要拒答**
- 将 chunk 用清晰标识包围（如 “【】”）
- 限制回答风格（条列式、简洁等）

更高级的 Prompt：

- “必须引用来源页码”
- “回答不超过 200 字”
- “必须保持与原文一致，不得改写规范内容”

一句话：

> **Chunk 保证你检索得准，Prompt 保证你回答得对。缺一不可。**





#### 14，如何让检索只限定在某一本规范/某一类文档中？（元数据过滤）

有些场景用户只想查：

- 《钢结构规范》
- 或《施工安全手册》
- 或“仅查桩基相关规范”

这时必须使用 Qdrant 的 **metadata filter**。

在写入 chunk 时，你会存：

```json
payload: {
  "pdf": "钢结构规范.pdf",
  "page": 400,
  "chapter": "第二章 焊接要求"
}
```

检索时加 filter：

```json
"filter": {
  "must": [
    { "key": "pdf", "match": "钢结构规范.pdf" }
  ]
}
```

效果：

- embedding 只和本规范比对
- 避免“跨规范误检索”

一句话总结：

> **元数据过滤 = 限定检索范围。
>  没有它，系统容易拿错规范回答。**



#### 15，返回 PDF 页面截图时如何避免重复渲染？如何设计页面缓存策略？

PDF → 图片截图是非常耗时的操作，不可能在用户提问时实时渲染。

正确做法：

**① 构建向量库阶段就把 PDF 每页预转成 PNG/JPG**

如：

```
steel/1.png
steel/2.png
...
steel/600.png
```

并写入 chunk payload：

```
"img": "/pdf_images/steel/400.png"
```

**② 图片托管在静态服务器/Nginx/CDN 上**

减少 Python 和 Java 的负担。

**③ 浏览器缓存（Cache-Control）**

避免重复加载同一张页图。

**④ 如果 PDF 更新，只需重新生成相关页面**

不用重渲染全部图片。

一句话总结：

> **“提前渲染 + 缓存 + 静态托管”是 RAG 系统里返回 PDF 页图的正确方式。**